{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CNN model with orthogonal encoding, window size 17\n",
    "In this model, we converted the sst3 sequence to orthogonal encoding and treated each character in the encoding as a feature. The window was chosen based on the results of experiments in related work. Since each encoding character is a feature, the amount of data grows very fast and a limit 'd' is set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import warnings\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Conv2D, MaxPool2D, Reshape, BatchNormalization, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import os\n",
    "import tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/2018-06-06-pdb-intersect-pisces.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "window =17\n",
    "aminoacids = 'ACDEFGHIKLMNPQRSTVWY*'\n",
    "n = len(aminoacids)\n",
    "matrice = np.identity(n)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "aa_dict = {}\n",
    "i = 0\n",
    "for aa in aminoacids:\n",
    "    aa_dict[aa] = matrice[i,]\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "    a0a0  a0a1  a0a2  a0a3  a0a4  a0a5  a0a6  a0a7  a0a8  a0a9  ...  a16a12  \\\n0    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     1.0   \n1    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     1.0   \n2    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n3    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n4    0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n5    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  ...     0.0   \n6    0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n7    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  ...     0.0   \n8    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n9    0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n10   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n11   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n12   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n13   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n14   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n15   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n16   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n17   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n18   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n19   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n20   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n21   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n22   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n23   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n24   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n25   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n26   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n27   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  ...     0.0   \n28   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n29   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n30   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...     0.0   \n\n    a16a13  a16a14  a16a15  a16a16  a16a17  a16a18  a16a19  a16a20  sst3  \n0      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n1      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n2      0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     2  \n3      1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n4      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n5      1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     1  \n6      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n7      0.0     1.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n8      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n9      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n10     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n11     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n12     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n13     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n14     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n15     0.0     0.0     0.0     0.0     0.0     0.0     0.0     1.0     2  \n16     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n17     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n18     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n19     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n20     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n21     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n22     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n23     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0  \n24     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n25     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0  \n26     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n27     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     2  \n28     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0     2  \n29     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0  \n30     0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     0  \n\n[31 rows x 358 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a0a0</th>\n      <th>a0a1</th>\n      <th>a0a2</th>\n      <th>a0a3</th>\n      <th>a0a4</th>\n      <th>a0a5</th>\n      <th>a0a6</th>\n      <th>a0a7</th>\n      <th>a0a8</th>\n      <th>a0a9</th>\n      <th>...</th>\n      <th>a16a12</th>\n      <th>a16a13</th>\n      <th>a16a14</th>\n      <th>a16a15</th>\n      <th>a16a16</th>\n      <th>a16a17</th>\n      <th>a16a18</th>\n      <th>a16a19</th>\n      <th>a16a20</th>\n      <th>sst3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>31 rows × 358 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdict = {}\n",
    "codes = []\n",
    "d = 30000\n",
    "a = 0\n",
    "data = {'H':0, 'E':0, 'C':0}\n",
    "while (data['H'] + data['E'] + data['C']) < d:\n",
    "    sequence = df.loc[a,'seq']\n",
    "    seq_sst3 = df.loc[a,'sst3']\n",
    "    i = 0\n",
    "    while i <= (len(sequence)-window):\n",
    "        j = 0\n",
    "        if seq_sst3[int((i+i+window)/2)] == 'H':\n",
    "            for character in sequence[i:i+window]:\n",
    "                for k in range(n):\n",
    "                    tempdict[f'a{j}a{k}'] = aa_dict[character][k]\n",
    "                    k +=1\n",
    "                j += 1\n",
    "            tempdict['sst3'] = 0\n",
    "            codes.append(tempdict)\n",
    "            tempdict = {}\n",
    "            i += 1\n",
    "            data['H'] += 1\n",
    "        elif seq_sst3[int((i+i+window)/2)] == 'E':\n",
    "            for character in sequence[i:i+window]:\n",
    "                for k in range(n):\n",
    "                    tempdict[f'a{j}a{k}'] = aa_dict[character][k]\n",
    "                    k +=1\n",
    "                j += 1\n",
    "            tempdict['sst3'] = 1\n",
    "            codes.append(tempdict)\n",
    "            tempdict = {}\n",
    "            i += 1\n",
    "            data['E'] += 1\n",
    "        elif seq_sst3[int((i+i+window)/2)] == 'C':\n",
    "            for character in sequence[i:i+window]:\n",
    "                for k in range(n):\n",
    "                    tempdict[f'a{j}a{k}'] = aa_dict[character][k]\n",
    "                    k +=1\n",
    "                j += 1\n",
    "            tempdict['sst3'] = 2\n",
    "            codes.append(tempdict)\n",
    "            tempdict = {}\n",
    "            i += 1\n",
    "            data['C'] += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    a += 1\n",
    "df2 = pd.DataFrame(codes)\n",
    "df2.loc[:30]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X = df2.drop(['sst3'], axis=1)\n",
    "y = df2['sst3']\n",
    "# Normalize the data\n",
    "cols = X.columns\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=[cols])\n",
    "# Reshape\n",
    "X = X.values.reshape(-1,window,21)\n",
    "# Label Encoding\n",
    "y = to_categorical(y, num_classes = 3)\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.25, random_state=0)\n",
    "\n",
    "LR = 0.0009\n",
    "drop_out = 0.38\n",
    "batch_dim = 64\n",
    "nn_epochs = 45\n",
    "loss = 'categorical_crossentropy'\n",
    "do_summary = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    np.random.seed(0)\n",
    "    tensorflow.random.set_seed(0)\n",
    "    random.seed(0)\n",
    "    os.environ['PYTHONHASHSEED']=str(0)\n",
    "    m = Sequential()\n",
    "    m.add(Conv1D(128, 11, padding='same', activation='relu', input_shape=(window, 21)))  # <----\n",
    "    m.add(Dropout(drop_out, seed=0))\n",
    "    m.add(Conv1D(64, 11, padding='same', activation='relu'))\n",
    "    m.add(Dropout(drop_out, seed=0))\n",
    "    m.add(Conv1D(3, 11, padding='same', activation='relu'))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(3, activation='softmax'))# <----\n",
    "    opt = optimizers.Adam(lr=LR)\n",
    "    m.compile(optimizer=opt,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy', 'mae', 'mse'])\n",
    "    m.build()\n",
    "    if do_summary:\n",
    "        print(\"\\nHyper Parameters\\n\")\n",
    "        print(\"Learning Rate: \" + str(LR))\n",
    "        print(\"Drop out: \" + str(drop_out))\n",
    "        print(\"Batch dim: \" + str(batch_dim))\n",
    "        print(\"Number of epochs: \" + str(nn_epochs))\n",
    "        print(\"\\nLoss: \" + loss + \"\\n\")\n",
    "\n",
    "        m.summary()\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#set early stopping criteria\n",
    "pat = 5 #this is the number of epochs with no improvement after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "\n",
    "#define the model checkpoint callback -> this will keep on saving the model as a physical file\n",
    "model_checkpoint = ModelCheckpoint('fas_mnist_1.h5', verbose=1, save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_78 (Conv1D)          (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_79 (Conv1D)          (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_80 (Conv1D)          (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten_26 (Flatten)        (None, 51)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.8348 - accuracy: 0.6238 - mae: 0.3234\n",
      "Epoch 1: val_loss improved from inf to 0.71982, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 8s 24ms/step - loss: 0.8339 - accuracy: 0.6243 - mae: 0.3232 - val_loss: 0.7198 - val_accuracy: 0.6953 - val_mae: 0.2904\n",
      "Epoch 2/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6953 - accuracy: 0.7040 - mae: 0.2670\n",
      "Epoch 2: val_loss improved from 0.71982 to 0.63983, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.6948 - accuracy: 0.7043 - mae: 0.2669 - val_loss: 0.6398 - val_accuracy: 0.7342 - val_mae: 0.2552\n",
      "Epoch 3/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6206 - accuracy: 0.7443 - mae: 0.2385\n",
      "Epoch 3: val_loss improved from 0.63983 to 0.58965, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.6206 - accuracy: 0.7443 - mae: 0.2385 - val_loss: 0.5897 - val_accuracy: 0.7618 - val_mae: 0.2366\n",
      "Epoch 4/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7673 - mae: 0.2178\n",
      "Epoch 4: val_loss improved from 0.58965 to 0.55075, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.5651 - accuracy: 0.7673 - mae: 0.2179 - val_loss: 0.5507 - val_accuracy: 0.7746 - val_mae: 0.2184\n",
      "Epoch 5/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.7860 - mae: 0.2023\n",
      "Epoch 5: val_loss improved from 0.55075 to 0.52558, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.5245 - accuracy: 0.7860 - mae: 0.2023 - val_loss: 0.5256 - val_accuracy: 0.7859 - val_mae: 0.2123\n",
      "Epoch 6/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4968 - accuracy: 0.7934 - mae: 0.1913\n",
      "Epoch 6: val_loss improved from 0.52558 to 0.50702, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.4969 - accuracy: 0.7936 - mae: 0.1912 - val_loss: 0.5070 - val_accuracy: 0.7941 - val_mae: 0.2068\n",
      "Epoch 7/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4704 - accuracy: 0.8093 - mae: 0.1812\n",
      "Epoch 7: val_loss improved from 0.50702 to 0.49238, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.4705 - accuracy: 0.8089 - mae: 0.1813 - val_loss: 0.4924 - val_accuracy: 0.8009 - val_mae: 0.1969\n",
      "Epoch 8/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.8190 - mae: 0.1724\n",
      "Epoch 8: val_loss improved from 0.49238 to 0.47243, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.4475 - accuracy: 0.8190 - mae: 0.1724 - val_loss: 0.4724 - val_accuracy: 0.8087 - val_mae: 0.1840\n",
      "Epoch 9/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8273 - mae: 0.1637\n",
      "Epoch 9: val_loss improved from 0.47243 to 0.45132, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.4259 - accuracy: 0.8273 - mae: 0.1638 - val_loss: 0.4513 - val_accuracy: 0.8177 - val_mae: 0.1714\n",
      "Epoch 10/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8314 - mae: 0.1595\n",
      "Epoch 10: val_loss improved from 0.45132 to 0.44585, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.4140 - accuracy: 0.8311 - mae: 0.1596 - val_loss: 0.4458 - val_accuracy: 0.8210 - val_mae: 0.1708\n",
      "Epoch 11/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3899 - accuracy: 0.8424 - mae: 0.1500\n",
      "Epoch 11: val_loss improved from 0.44585 to 0.43664, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3897 - accuracy: 0.8425 - mae: 0.1500 - val_loss: 0.4366 - val_accuracy: 0.8242 - val_mae: 0.1709\n",
      "Epoch 12/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.8476 - mae: 0.1448\n",
      "Epoch 12: val_loss improved from 0.43664 to 0.42210, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3763 - accuracy: 0.8476 - mae: 0.1448 - val_loss: 0.4221 - val_accuracy: 0.8303 - val_mae: 0.1648\n",
      "Epoch 13/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3570 - accuracy: 0.8581 - mae: 0.1378\n",
      "Epoch 13: val_loss did not improve from 0.42210\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.3572 - accuracy: 0.8579 - mae: 0.1378 - val_loss: 0.4251 - val_accuracy: 0.8315 - val_mae: 0.1681\n",
      "Epoch 14/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8555 - mae: 0.1379\n",
      "Epoch 14: val_loss did not improve from 0.42210\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.3599 - accuracy: 0.8555 - mae: 0.1379 - val_loss: 0.4223 - val_accuracy: 0.8312 - val_mae: 0.1613\n",
      "Epoch 15/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8655 - mae: 0.1295\n",
      "Epoch 15: val_loss improved from 0.42210 to 0.40840, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3381 - accuracy: 0.8652 - mae: 0.1297 - val_loss: 0.4084 - val_accuracy: 0.8357 - val_mae: 0.1493\n",
      "Epoch 16/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8691 - mae: 0.1256\n",
      "Epoch 16: val_loss did not improve from 0.40840\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3281 - accuracy: 0.8691 - mae: 0.1256 - val_loss: 0.4142 - val_accuracy: 0.8365 - val_mae: 0.1628\n",
      "Epoch 17/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8729 - mae: 0.1227\n",
      "Epoch 17: val_loss did not improve from 0.40840\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3191 - accuracy: 0.8729 - mae: 0.1227 - val_loss: 0.4180 - val_accuracy: 0.8380 - val_mae: 0.1558\n",
      "Epoch 18/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.8747 - mae: 0.1203\n",
      "Epoch 18: val_loss did not improve from 0.40840\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3140 - accuracy: 0.8747 - mae: 0.1203 - val_loss: 0.4132 - val_accuracy: 0.8402 - val_mae: 0.1511\n",
      "Epoch 19/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3113 - accuracy: 0.8757 - mae: 0.1186\n",
      "Epoch 19: val_loss improved from 0.40840 to 0.40498, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3113 - accuracy: 0.8756 - mae: 0.1186 - val_loss: 0.4050 - val_accuracy: 0.8421 - val_mae: 0.1477\n",
      "Epoch 20/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8848 - mae: 0.1125\n",
      "Epoch 20: val_loss improved from 0.40498 to 0.40035, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2933 - accuracy: 0.8847 - mae: 0.1125 - val_loss: 0.4003 - val_accuracy: 0.8428 - val_mae: 0.1439\n",
      "Epoch 21/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.8853 - mae: 0.1105\n",
      "Epoch 21: val_loss did not improve from 0.40035\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2897 - accuracy: 0.8852 - mae: 0.1106 - val_loss: 0.4057 - val_accuracy: 0.8377 - val_mae: 0.1493\n",
      "Epoch 22/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.8888 - mae: 0.1088\n",
      "Epoch 22: val_loss did not improve from 0.40035\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2863 - accuracy: 0.8886 - mae: 0.1088 - val_loss: 0.4006 - val_accuracy: 0.8430 - val_mae: 0.1449\n",
      "Epoch 23/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.8881 - mae: 0.1063\n",
      "Epoch 23: val_loss improved from 0.40035 to 0.39541, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.2796 - accuracy: 0.8881 - mae: 0.1063 - val_loss: 0.3954 - val_accuracy: 0.8463 - val_mae: 0.1414\n",
      "Epoch 24/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.8925 - mae: 0.1029\n",
      "Epoch 24: val_loss improved from 0.39541 to 0.39174, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2705 - accuracy: 0.8925 - mae: 0.1029 - val_loss: 0.3917 - val_accuracy: 0.8448 - val_mae: 0.1374\n",
      "Epoch 25/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.8984 - mae: 0.0999\n",
      "Epoch 25: val_loss improved from 0.39174 to 0.38856, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2600 - accuracy: 0.8984 - mae: 0.0999 - val_loss: 0.3886 - val_accuracy: 0.8485 - val_mae: 0.1397\n",
      "Epoch 26/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9021 - mae: 0.0956\n",
      "Epoch 26: val_loss improved from 0.38856 to 0.38673, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2548 - accuracy: 0.9021 - mae: 0.0956 - val_loss: 0.3867 - val_accuracy: 0.8470 - val_mae: 0.1405\n",
      "Epoch 27/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.8995 - mae: 0.0965\n",
      "Epoch 27: val_loss improved from 0.38673 to 0.38405, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2536 - accuracy: 0.8995 - mae: 0.0965 - val_loss: 0.3841 - val_accuracy: 0.8506 - val_mae: 0.1359\n",
      "Epoch 28/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9008 - mae: 0.0959\n",
      "Epoch 28: val_loss did not improve from 0.38405\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.2554 - accuracy: 0.9008 - mae: 0.0959 - val_loss: 0.3937 - val_accuracy: 0.8451 - val_mae: 0.1390\n",
      "Epoch 29/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9025 - mae: 0.0937\n",
      "Epoch 29: val_loss did not improve from 0.38405\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2461 - accuracy: 0.9025 - mae: 0.0937 - val_loss: 0.4012 - val_accuracy: 0.8433 - val_mae: 0.1410\n",
      "Epoch 30/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2432 - accuracy: 0.9039 - mae: 0.0927\n",
      "Epoch 30: val_loss did not improve from 0.38405\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2425 - accuracy: 0.9042 - mae: 0.0925 - val_loss: 0.3888 - val_accuracy: 0.8471 - val_mae: 0.1317\n",
      "Epoch 31/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9070 - mae: 0.0898\n",
      "Epoch 31: val_loss improved from 0.38405 to 0.38249, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2409 - accuracy: 0.9070 - mae: 0.0898 - val_loss: 0.3825 - val_accuracy: 0.8536 - val_mae: 0.1262\n",
      "Epoch 32/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.9066 - mae: 0.0890\n",
      "Epoch 32: val_loss improved from 0.38249 to 0.38125, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.2369 - accuracy: 0.9066 - mae: 0.0891 - val_loss: 0.3812 - val_accuracy: 0.8555 - val_mae: 0.1320\n",
      "Epoch 33/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2294 - accuracy: 0.9099 - mae: 0.0865\n",
      "Epoch 33: val_loss did not improve from 0.38125\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.2298 - accuracy: 0.9097 - mae: 0.0866 - val_loss: 0.3821 - val_accuracy: 0.8515 - val_mae: 0.1331\n",
      "Epoch 34/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9118 - mae: 0.0870\n",
      "Epoch 34: val_loss did not improve from 0.38125\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2312 - accuracy: 0.9118 - mae: 0.0870 - val_loss: 0.3879 - val_accuracy: 0.8491 - val_mae: 0.1294\n",
      "Epoch 35/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2322 - accuracy: 0.9103 - mae: 0.0876\n",
      "Epoch 35: val_loss improved from 0.38125 to 0.37761, saving model to fas_mnist_1.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2324 - accuracy: 0.9103 - mae: 0.0876 - val_loss: 0.3776 - val_accuracy: 0.8548 - val_mae: 0.1331\n",
      "Epoch 36/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.9161 - mae: 0.0816\n",
      "Epoch 36: val_loss did not improve from 0.37761\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2166 - accuracy: 0.9162 - mae: 0.0816 - val_loss: 0.3877 - val_accuracy: 0.8501 - val_mae: 0.1318\n",
      "Epoch 37/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.9148 - mae: 0.0810\n",
      "Epoch 37: val_loss did not improve from 0.37761\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.2135 - accuracy: 0.9148 - mae: 0.0810 - val_loss: 0.3859 - val_accuracy: 0.8540 - val_mae: 0.1330\n",
      "Epoch 38/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2166 - accuracy: 0.9162 - mae: 0.0816\n",
      "Epoch 38: val_loss did not improve from 0.37761\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.2177 - accuracy: 0.9157 - mae: 0.0819 - val_loss: 0.3947 - val_accuracy: 0.8483 - val_mae: 0.1260\n",
      "Epoch 39/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9181 - mae: 0.0795\n",
      "Epoch 39: val_loss did not improve from 0.37761\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2110 - accuracy: 0.9181 - mae: 0.0795 - val_loss: 0.3824 - val_accuracy: 0.8548 - val_mae: 0.1269\n",
      "Epoch 40/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9230 - mae: 0.0749\n",
      "Epoch 40: val_loss did not improve from 0.37761\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.2002 - accuracy: 0.9230 - mae: 0.0749 - val_loss: 0.3960 - val_accuracy: 0.8518 - val_mae: 0.1295\n",
      "Epoch 40: early stopping\n",
      "\n",
      "\n",
      "Time elapsed: 265.84 s\n",
      "188/188 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "model = CNN_model()\n",
    "history = model.fit(X_train, Y_train, epochs= nn_epochs, callbacks=[early_stopping, model_checkpoint], batch_size= batch_dim, shuffle=True,\n",
    "                        validation_data=(X_val, Y_val))\n",
    "end_time = timer()\n",
    "print(\"\\n\\nTime elapsed: \" + \"{0:.2f}\".format((end_time - start_time)) + \" s\")\n",
    "predictions = model.predict(X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'Epochs')"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 500x800 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.sans-serif'] = \"Times New Roman\"\n",
    "fig, ax = plt.subplots(2,1, figsize=(5,8))\n",
    "# Plot the loss and accuracy curves for training and validation\n",
    "ax[0].plot(np.arange(1,len(history.history['val_loss'])+1),history.history['val_loss'], label=\"Validation loss\", c='#000067')\n",
    "ax[0].plot(np.arange(1,len(history.history['loss'])+1),history.history['loss'], label=\"Training loss\", c='#AAAAFF')\n",
    "ax[0].plot(35, history.history['val_loss'][34], 'go', markersize=3, c='r', label='Model Checkpoint')\n",
    "ax[0].legend(fontsize='small')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].plot(np.arange(1,len(history.history['val_accuracy'])+1),history.history['val_accuracy'], label=\"Validation accuracy\", c='#000067')\n",
    "ax[1].plot(np.arange(1,len(history.history['accuracy'])+1),history.history['accuracy'], label=\"Training accuracy\", c='#AAAAFF')\n",
    "ax[1].plot(35, history.history['val_accuracy'][34], 'go', markersize=3, c='r', label='Model Checkpoint')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend(fontsize='small')\n",
    "plt.xlabel('Epochs')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92      3034\n",
      "           1       0.77      0.69      0.73       888\n",
      "           2       0.76      0.83      0.80      2090\n",
      "\n",
      "    accuracy                           0.85      6012\n",
      "   macro avg       0.82      0.81      0.81      6012\n",
      "weighted avg       0.85      0.85      0.85      6012\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 500x400 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "model = load_model('fas_mnist_1.h5')\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
    "Y_true = np.argmax(Y_test,axis = 1)\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"YlGnBu\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "print(classification_report(Y_true, Y_pred_classes))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score with CNN classifier: 0.8453\n",
      "564/564 [==============================] - 2s 4ms/step\n",
      "Training set score: 0.9889\n",
      "Null accuracy score: 0.5047\n"
     ]
    }
   ],
   "source": [
    "# compute and print accuracy score\n",
    "print('Model accuracy score with CNN classifier: {0:0.4f}'. format(accuracy_score(Y_true, Y_pred_classes)))\n",
    "# check for overfitting and underfitting\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_train_pred_classes = np.argmax(Y_train_pred,axis = 1)\n",
    "Y_train_true = np.argmax(Y_train,axis = 1)\n",
    "print('Training set score: {:.4f}'.format(accuracy_score(Y_train_true, Y_train_pred_classes)))\n",
    "# compare model accuracy with null accuracy\n",
    "Y_true_df = pd.DataFrame(Y_true)\n",
    "null_accuracy = (max(Y_true_df.value_counts())/(sum(Y_true_df.value_counts())))\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28/282 [=>............................] - ETA: 5s - loss: 0.2518 - accuracy: 0.8990 - mae: 0.0994 - mse: 0.0478"
     ]
    }
   ],
   "source": [
    "def fit_and_evaluate(t_x, test_x, t_y, test_y, EPOCHS, BATCH_SIZE):\n",
    "    model = None\n",
    "    model = CNN_model()\n",
    "    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint],\n",
    "              verbose=1, validation_split=0.25)\n",
    "    val_scor = model.evaluate(test_x, test_y)\n",
    "    print(\"Val Score: \", val_scor)\n",
    "    return val_scor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold:  1\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 51)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.8387 - accuracy: 0.6190 - mae: 0.3253 - mse: 0.1641\n",
      "Epoch 1: val_loss improved from inf to 0.74529, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 9s 25ms/step - loss: 0.8387 - accuracy: 0.6190 - mae: 0.3253 - mse: 0.1641 - val_loss: 0.7453 - val_accuracy: 0.6893 - val_mae: 0.3053 - val_mse: 0.1439\n",
      "Epoch 2/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.7092 - mae: 0.2656 - mse: 0.1332\n",
      "Epoch 2: val_loss improved from 0.74529 to 0.65474, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.6910 - accuracy: 0.7096 - mae: 0.2655 - mse: 0.1331 - val_loss: 0.6547 - val_accuracy: 0.7292 - val_mae: 0.2683 - val_mse: 0.1257\n",
      "Epoch 3/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.6104 - accuracy: 0.7461 - mae: 0.2354 - mse: 0.1178\n",
      "Epoch 3: val_loss improved from 0.65474 to 0.59410, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 21ms/step - loss: 0.6106 - accuracy: 0.7462 - mae: 0.2355 - mse: 0.1178 - val_loss: 0.5941 - val_accuracy: 0.7596 - val_mae: 0.2348 - val_mse: 0.1142\n",
      "Epoch 4/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.7714 - mae: 0.2139 - mse: 0.1070\n",
      "Epoch 4: val_loss improved from 0.59410 to 0.56229, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.5546 - accuracy: 0.7714 - mae: 0.2139 - mse: 0.1070 - val_loss: 0.5623 - val_accuracy: 0.7705 - val_mae: 0.2195 - val_mse: 0.1077\n",
      "Epoch 5/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7839 - mae: 0.2011 - mse: 0.1006\n",
      "Epoch 5: val_loss improved from 0.56229 to 0.53575, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.5224 - accuracy: 0.7839 - mae: 0.2012 - mse: 0.1007 - val_loss: 0.5358 - val_accuracy: 0.7838 - val_mae: 0.2219 - val_mse: 0.1020\n",
      "Epoch 6/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4901 - accuracy: 0.8008 - mae: 0.1892 - mse: 0.0943\n",
      "Epoch 6: val_loss improved from 0.53575 to 0.51503, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.4901 - accuracy: 0.8008 - mae: 0.1892 - mse: 0.0943 - val_loss: 0.5150 - val_accuracy: 0.7929 - val_mae: 0.2103 - val_mse: 0.0984\n",
      "Epoch 7/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4648 - accuracy: 0.8092 - mae: 0.1793 - mse: 0.0896\n",
      "Epoch 7: val_loss improved from 0.51503 to 0.50151, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.4650 - accuracy: 0.8092 - mae: 0.1794 - mse: 0.0896 - val_loss: 0.5015 - val_accuracy: 0.7979 - val_mae: 0.2035 - val_mse: 0.0961\n",
      "Epoch 8/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.8244 - mae: 0.1682 - mse: 0.0837\n",
      "Epoch 8: val_loss improved from 0.50151 to 0.48009, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.4356 - accuracy: 0.8244 - mae: 0.1681 - mse: 0.0837 - val_loss: 0.4801 - val_accuracy: 0.8104 - val_mae: 0.1793 - val_mse: 0.0912\n",
      "Epoch 9/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4224 - accuracy: 0.8299 - mae: 0.1618 - mse: 0.0810\n",
      "Epoch 9: val_loss improved from 0.48009 to 0.47311, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 21ms/step - loss: 0.4217 - accuracy: 0.8303 - mae: 0.1616 - mse: 0.0809 - val_loss: 0.4731 - val_accuracy: 0.8134 - val_mae: 0.1900 - val_mse: 0.0903\n",
      "Epoch 10/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.8373 - mae: 0.1544 - mse: 0.0775\n",
      "Epoch 10: val_loss improved from 0.47311 to 0.46503, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 21ms/step - loss: 0.4027 - accuracy: 0.8373 - mae: 0.1544 - mse: 0.0775 - val_loss: 0.4650 - val_accuracy: 0.8129 - val_mae: 0.1838 - val_mse: 0.0884\n",
      "Epoch 11/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8472 - mae: 0.1470 - mse: 0.0732\n",
      "Epoch 11: val_loss improved from 0.46503 to 0.45071, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 21ms/step - loss: 0.3817 - accuracy: 0.8471 - mae: 0.1470 - mse: 0.0732 - val_loss: 0.4507 - val_accuracy: 0.8187 - val_mae: 0.1787 - val_mse: 0.0861\n",
      "Epoch 12/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8514 - mae: 0.1435 - mse: 0.0717\n",
      "Epoch 12: val_loss improved from 0.45071 to 0.44949, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3747 - accuracy: 0.8514 - mae: 0.1435 - mse: 0.0717 - val_loss: 0.4495 - val_accuracy: 0.8225 - val_mae: 0.1770 - val_mse: 0.0861\n",
      "Epoch 13/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.8582 - mae: 0.1375 - mse: 0.0685\n",
      "Epoch 13: val_loss improved from 0.44949 to 0.44809, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.3612 - accuracy: 0.8580 - mae: 0.1376 - mse: 0.0686 - val_loss: 0.4481 - val_accuracy: 0.8185 - val_mae: 0.1750 - val_mse: 0.0861\n",
      "Epoch 14/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3429 - accuracy: 0.8637 - mae: 0.1319 - mse: 0.0654\n",
      "Epoch 14: val_loss improved from 0.44809 to 0.42803, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.3429 - accuracy: 0.8636 - mae: 0.1319 - mse: 0.0654 - val_loss: 0.4280 - val_accuracy: 0.8322 - val_mae: 0.1701 - val_mse: 0.0820\n",
      "Epoch 15/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.8677 - mae: 0.1273 - mse: 0.0637\n",
      "Epoch 15: val_loss did not improve from 0.42803\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.3317 - accuracy: 0.8677 - mae: 0.1273 - mse: 0.0637 - val_loss: 0.4290 - val_accuracy: 0.8317 - val_mae: 0.1678 - val_mse: 0.0817\n",
      "Epoch 16/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8700 - mae: 0.1238 - mse: 0.0621\n",
      "Epoch 16: val_loss improved from 0.42803 to 0.42252, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3232 - accuracy: 0.8701 - mae: 0.1237 - mse: 0.0620 - val_loss: 0.4225 - val_accuracy: 0.8377 - val_mae: 0.1549 - val_mse: 0.0796\n",
      "Epoch 17/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8751 - mae: 0.1202 - mse: 0.0604\n",
      "Epoch 17: val_loss did not improve from 0.42252\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3166 - accuracy: 0.8747 - mae: 0.1203 - mse: 0.0604 - val_loss: 0.4247 - val_accuracy: 0.8332 - val_mae: 0.1574 - val_mse: 0.0804\n",
      "Epoch 18/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8782 - mae: 0.1165 - mse: 0.0582\n",
      "Epoch 18: val_loss improved from 0.42252 to 0.42048, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3050 - accuracy: 0.8784 - mae: 0.1165 - mse: 0.0582 - val_loss: 0.4205 - val_accuracy: 0.8313 - val_mae: 0.1541 - val_mse: 0.0797\n",
      "Epoch 19/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8824 - mae: 0.1142 - mse: 0.0572\n",
      "Epoch 19: val_loss did not improve from 0.42048\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.2998 - accuracy: 0.8823 - mae: 0.1142 - mse: 0.0572 - val_loss: 0.4337 - val_accuracy: 0.8278 - val_mae: 0.1539 - val_mse: 0.0812\n",
      "Epoch 20/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8867 - mae: 0.1099 - mse: 0.0554\n",
      "Epoch 20: val_loss improved from 0.42048 to 0.41411, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 10s 35ms/step - loss: 0.2913 - accuracy: 0.8867 - mae: 0.1099 - mse: 0.0554 - val_loss: 0.4141 - val_accuracy: 0.8385 - val_mae: 0.1573 - val_mse: 0.0786\n",
      "Epoch 21/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.8881 - mae: 0.1069 - mse: 0.0535\n",
      "Epoch 21: val_loss improved from 0.41411 to 0.41152, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 11s 38ms/step - loss: 0.2811 - accuracy: 0.8880 - mae: 0.1069 - mse: 0.0535 - val_loss: 0.4115 - val_accuracy: 0.8377 - val_mae: 0.1475 - val_mse: 0.0778\n",
      "Epoch 22/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.8912 - mae: 0.1055 - mse: 0.0530\n",
      "Epoch 22: val_loss did not improve from 0.41152\n",
      "282/282 [==============================] - 10s 35ms/step - loss: 0.2765 - accuracy: 0.8913 - mae: 0.1055 - mse: 0.0530 - val_loss: 0.4124 - val_accuracy: 0.8383 - val_mae: 0.1505 - val_mse: 0.0780\n",
      "Epoch 23/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2694 - accuracy: 0.8951 - mae: 0.1018 - mse: 0.0512\n",
      "Epoch 23: val_loss did not improve from 0.41152\n",
      "282/282 [==============================] - 11s 40ms/step - loss: 0.2690 - accuracy: 0.8953 - mae: 0.1017 - mse: 0.0511 - val_loss: 0.4147 - val_accuracy: 0.8358 - val_mae: 0.1501 - val_mse: 0.0785\n",
      "Epoch 24/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.8994 - mae: 0.0992 - mse: 0.0496\n",
      "Epoch 24: val_loss did not improve from 0.41152\n",
      "282/282 [==============================] - 11s 39ms/step - loss: 0.2603 - accuracy: 0.8997 - mae: 0.0991 - mse: 0.0495 - val_loss: 0.4276 - val_accuracy: 0.8328 - val_mae: 0.1523 - val_mse: 0.0813\n",
      "Epoch 25/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.8991 - mae: 0.0973 - mse: 0.0495\n",
      "Epoch 25: val_loss improved from 0.41152 to 0.40341, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 10s 34ms/step - loss: 0.2598 - accuracy: 0.8991 - mae: 0.0973 - mse: 0.0495 - val_loss: 0.4034 - val_accuracy: 0.8445 - val_mae: 0.1424 - val_mse: 0.0752\n",
      "Epoch 26/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2546 - accuracy: 0.9007 - mae: 0.0967 - mse: 0.0485\n",
      "Epoch 26: val_loss improved from 0.40341 to 0.40330, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 13s 46ms/step - loss: 0.2546 - accuracy: 0.9007 - mae: 0.0967 - mse: 0.0485 - val_loss: 0.4033 - val_accuracy: 0.8443 - val_mae: 0.1400 - val_mse: 0.0756\n",
      "Epoch 27/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9046 - mae: 0.0917 - mse: 0.0462\n",
      "Epoch 27: val_loss did not improve from 0.40330\n",
      "282/282 [==============================] - 10s 37ms/step - loss: 0.2433 - accuracy: 0.9046 - mae: 0.0917 - mse: 0.0462 - val_loss: 0.4156 - val_accuracy: 0.8367 - val_mae: 0.1417 - val_mse: 0.0782\n",
      "Epoch 28/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9057 - mae: 0.0922 - mse: 0.0466\n",
      "Epoch 28: val_loss improved from 0.40330 to 0.40314, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 10s 35ms/step - loss: 0.2468 - accuracy: 0.9057 - mae: 0.0922 - mse: 0.0466 - val_loss: 0.4031 - val_accuracy: 0.8486 - val_mae: 0.1389 - val_mse: 0.0746\n",
      "Epoch 29/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9069 - mae: 0.0904 - mse: 0.0458\n",
      "Epoch 29: val_loss improved from 0.40314 to 0.40297, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 10s 35ms/step - loss: 0.2424 - accuracy: 0.9070 - mae: 0.0903 - mse: 0.0457 - val_loss: 0.4030 - val_accuracy: 0.8412 - val_mae: 0.1389 - val_mse: 0.0755\n",
      "Epoch 30/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.9090 - mae: 0.0885 - mse: 0.0448\n",
      "Epoch 30: val_loss improved from 0.40297 to 0.40196, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.2355 - accuracy: 0.9090 - mae: 0.0885 - mse: 0.0448 - val_loss: 0.4020 - val_accuracy: 0.8475 - val_mae: 0.1338 - val_mse: 0.0746\n",
      "Epoch 31/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9107 - mae: 0.0871 - mse: 0.0437\n",
      "Epoch 31: val_loss did not improve from 0.40196\n",
      "282/282 [==============================] - 8s 30ms/step - loss: 0.2319 - accuracy: 0.9105 - mae: 0.0871 - mse: 0.0437 - val_loss: 0.4145 - val_accuracy: 0.8468 - val_mae: 0.1354 - val_mse: 0.0762\n",
      "Epoch 32/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9101 - mae: 0.0874 - mse: 0.0444\n",
      "Epoch 32: val_loss did not improve from 0.40196\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.2324 - accuracy: 0.9099 - mae: 0.0876 - mse: 0.0445 - val_loss: 0.4128 - val_accuracy: 0.8400 - val_mae: 0.1395 - val_mse: 0.0769\n",
      "Epoch 33/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9121 - mae: 0.0848 - mse: 0.0428\n",
      "Epoch 33: val_loss improved from 0.40196 to 0.39604, saving model to fas_mnist_1_6f.h5\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.2251 - accuracy: 0.9122 - mae: 0.0848 - mse: 0.0428 - val_loss: 0.3960 - val_accuracy: 0.8460 - val_mae: 0.1388 - val_mse: 0.0743\n",
      "Epoch 34/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2209 - accuracy: 0.9134 - mae: 0.0837 - mse: 0.0419\n",
      "Epoch 34: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2220 - accuracy: 0.9130 - mae: 0.0840 - mse: 0.0421 - val_loss: 0.4053 - val_accuracy: 0.8458 - val_mae: 0.1337 - val_mse: 0.0753\n",
      "Epoch 35/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9185 - mae: 0.0795 - mse: 0.0399\n",
      "Epoch 35: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2122 - accuracy: 0.9185 - mae: 0.0795 - mse: 0.0399 - val_loss: 0.4022 - val_accuracy: 0.8453 - val_mae: 0.1374 - val_mse: 0.0746\n",
      "Epoch 36/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9209 - mae: 0.0773 - mse: 0.0391\n",
      "Epoch 36: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2084 - accuracy: 0.9209 - mae: 0.0774 - mse: 0.0391 - val_loss: 0.4060 - val_accuracy: 0.8480 - val_mae: 0.1317 - val_mse: 0.0747\n",
      "Epoch 37/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9199 - mae: 0.0775 - mse: 0.0393\n",
      "Epoch 37: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2047 - accuracy: 0.9199 - mae: 0.0775 - mse: 0.0393 - val_loss: 0.4126 - val_accuracy: 0.8441 - val_mae: 0.1333 - val_mse: 0.0763\n",
      "Epoch 38/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9199 - mae: 0.0764 - mse: 0.0390\n",
      "Epoch 38: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2047 - accuracy: 0.9200 - mae: 0.0764 - mse: 0.0390 - val_loss: 0.4172 - val_accuracy: 0.8430 - val_mae: 0.1331 - val_mse: 0.0767\n",
      "Epoch 38: early stopping\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.4068 - accuracy: 0.8521 - mae: 0.1281 - mse: 0.0736\n",
      "Val Score:  [0.4067743122577667, 0.8521291017532349, 0.1280788779258728, 0.0736314207315445]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  2\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 51)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.8202 - accuracy: 0.6365 - mae: 0.3169 - mse: 0.1597\n",
      "Epoch 1: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 27ms/step - loss: 0.8196 - accuracy: 0.6368 - mae: 0.3168 - mse: 0.1596 - val_loss: 0.7131 - val_accuracy: 0.7039 - val_mae: 0.2918 - val_mse: 0.1376\n",
      "Epoch 2/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.7159 - mae: 0.2604 - mse: 0.1304\n",
      "Epoch 2: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.6770 - accuracy: 0.7162 - mae: 0.2602 - mse: 0.1303 - val_loss: 0.6272 - val_accuracy: 0.7414 - val_mae: 0.2568 - val_mse: 0.1210\n",
      "Epoch 3/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154\n",
      "Epoch 3: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154 - val_loss: 0.5801 - val_accuracy: 0.7567 - val_mae: 0.2277 - val_mse: 0.1125\n",
      "Epoch 4/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1060\n",
      "Epoch 4: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061 - val_loss: 0.5563 - val_accuracy: 0.7680 - val_mae: 0.2159 - val_mse: 0.1071\n",
      "Epoch 5/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996\n",
      "Epoch 5: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996 - val_loss: 0.5310 - val_accuracy: 0.7846 - val_mae: 0.2024 - val_mse: 0.1025\n",
      "Epoch 6/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4890 - accuracy: 0.7979 - mae: 0.1894 - mse: 0.0946\n",
      "Epoch 6: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947 - val_loss: 0.5237 - val_accuracy: 0.7876 - val_mae: 0.2042 - val_mse: 0.1006\n",
      "Epoch 7/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870\n",
      "Epoch 7: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870 - val_loss: 0.4840 - val_accuracy: 0.8034 - val_mae: 0.1841 - val_mse: 0.0922\n",
      "Epoch 8/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8244 - mae: 0.1668 - mse: 0.0829\n",
      "Epoch 8: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.4294 - accuracy: 0.8244 - mae: 0.1668 - mse: 0.0829 - val_loss: 0.4868 - val_accuracy: 0.8062 - val_mae: 0.2001 - val_mse: 0.0929\n",
      "Epoch 9/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8337 - mae: 0.1592 - mse: 0.0793\n",
      "Epoch 9: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.4113 - accuracy: 0.8334 - mae: 0.1593 - mse: 0.0794 - val_loss: 0.4809 - val_accuracy: 0.8056 - val_mae: 0.1955 - val_mse: 0.0918\n",
      "Epoch 10/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8391 - mae: 0.1530 - mse: 0.0761\n",
      "Epoch 10: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3982 - accuracy: 0.8390 - mae: 0.1531 - mse: 0.0761 - val_loss: 0.4519 - val_accuracy: 0.8169 - val_mae: 0.1785 - val_mse: 0.0869\n",
      "Epoch 11/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731\n",
      "Epoch 11: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731 - val_loss: 0.4487 - val_accuracy: 0.8235 - val_mae: 0.1769 - val_mse: 0.0858\n",
      "Epoch 12/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.8527 - mae: 0.1399 - mse: 0.0698\n",
      "Epoch 12: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698 - val_loss: 0.4570 - val_accuracy: 0.8199 - val_mae: 0.1743 - val_mse: 0.0876\n",
      "Epoch 13/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.8613 - mae: 0.1328 - mse: 0.0661\n",
      "Epoch 13: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3465 - accuracy: 0.8613 - mae: 0.1328 - mse: 0.0661 - val_loss: 0.4406 - val_accuracy: 0.8270 - val_mae: 0.1688 - val_mse: 0.0842\n",
      "Epoch 14/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8652 - mae: 0.1293 - mse: 0.0645\n",
      "Epoch 14: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.3365 - accuracy: 0.8646 - mae: 0.1296 - mse: 0.0647 - val_loss: 0.4417 - val_accuracy: 0.8217 - val_mae: 0.1649 - val_mse: 0.0841\n",
      "Epoch 15/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8687 - mae: 0.1256 - mse: 0.0621\n",
      "Epoch 15: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3224 - accuracy: 0.8687 - mae: 0.1255 - mse: 0.0621 - val_loss: 0.4524 - val_accuracy: 0.8244 - val_mae: 0.1558 - val_mse: 0.0849\n",
      "Epoch 16/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8746 - mae: 0.1211 - mse: 0.0606\n",
      "Epoch 16: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607 - val_loss: 0.4260 - val_accuracy: 0.8328 - val_mae: 0.1710 - val_mse: 0.0812\n",
      "Epoch 17/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8760 - mae: 0.1188 - mse: 0.0589\n",
      "Epoch 17: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.3062 - accuracy: 0.8760 - mae: 0.1188 - mse: 0.0589 - val_loss: 0.4412 - val_accuracy: 0.8255 - val_mae: 0.1547 - val_mse: 0.0830\n",
      "Epoch 18/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8803 - mae: 0.1136 - mse: 0.0573\n",
      "Epoch 18: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573 - val_loss: 0.4224 - val_accuracy: 0.8310 - val_mae: 0.1597 - val_mse: 0.0804\n",
      "Epoch 19/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.8867 - mae: 0.1112 - mse: 0.0553\n",
      "Epoch 19: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552 - val_loss: 0.4237 - val_accuracy: 0.8358 - val_mae: 0.1463 - val_mse: 0.0794\n",
      "Epoch 20/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.8850 - mae: 0.1100 - mse: 0.0555\n",
      "Epoch 20: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 30ms/step - loss: 0.2880 - accuracy: 0.8850 - mae: 0.1100 - mse: 0.0555 - val_loss: 0.4052 - val_accuracy: 0.8440 - val_mae: 0.1515 - val_mse: 0.0765\n",
      "Epoch 21/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8901 - mae: 0.1056 - mse: 0.0533\n",
      "Epoch 21: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2783 - accuracy: 0.8903 - mae: 0.1055 - mse: 0.0533 - val_loss: 0.4240 - val_accuracy: 0.8358 - val_mae: 0.1556 - val_mse: 0.0801\n",
      "Epoch 22/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.8917 - mae: 0.1035 - mse: 0.0520\n",
      "Epoch 22: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520 - val_loss: 0.3986 - val_accuracy: 0.8425 - val_mae: 0.1510 - val_mse: 0.0759\n",
      "Epoch 23/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.8977 - mae: 0.1005 - mse: 0.0504\n",
      "Epoch 23: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2632 - accuracy: 0.8979 - mae: 0.1004 - mse: 0.0503 - val_loss: 0.4111 - val_accuracy: 0.8446 - val_mae: 0.1445 - val_mse: 0.0772\n",
      "Epoch 24/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8977 - mae: 0.0980 - mse: 0.0498\n",
      "Epoch 24: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2595 - accuracy: 0.8978 - mae: 0.0979 - mse: 0.0498 - val_loss: 0.4065 - val_accuracy: 0.8441 - val_mae: 0.1451 - val_mse: 0.0761\n",
      "Epoch 25/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9016 - mae: 0.0951 - mse: 0.0478\n",
      "Epoch 25: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 27ms/step - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479 - val_loss: 0.4008 - val_accuracy: 0.8475 - val_mae: 0.1382 - val_mse: 0.0746\n",
      "Epoch 26/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9000 - mae: 0.0948 - mse: 0.0479\n",
      "Epoch 26: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.2479 - accuracy: 0.9001 - mae: 0.0948 - mse: 0.0478 - val_loss: 0.4208 - val_accuracy: 0.8413 - val_mae: 0.1401 - val_mse: 0.0782\n",
      "Epoch 27/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9034 - mae: 0.0932 - mse: 0.0471\n",
      "Epoch 27: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2490 - accuracy: 0.9035 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.4033 - val_accuracy: 0.8405 - val_mae: 0.1423 - val_mse: 0.0760\n",
      "Epoch 27: early stopping\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.4113 - accuracy: 0.8373 - mae: 0.1439 - mse: 0.0774\n",
      "Val Score:  [0.41132697463035583, 0.8373253345489502, 0.14391790330410004, 0.0773804634809494]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  3\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_6 (Conv1D)           (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 51)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.8202 - accuracy: 0.6365 - mae: 0.3169 - mse: 0.1597\n",
      "Epoch 1: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 25ms/step - loss: 0.8196 - accuracy: 0.6368 - mae: 0.3168 - mse: 0.1596 - val_loss: 0.7131 - val_accuracy: 0.7039 - val_mae: 0.2918 - val_mse: 0.1376\n",
      "Epoch 2/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.7159 - mae: 0.2604 - mse: 0.1304\n",
      "Epoch 2: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.6770 - accuracy: 0.7162 - mae: 0.2602 - mse: 0.1303 - val_loss: 0.6272 - val_accuracy: 0.7414 - val_mae: 0.2568 - val_mse: 0.1210\n",
      "Epoch 3/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.7512 - mae: 0.2307 - mse: 0.1154\n",
      "Epoch 3: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154 - val_loss: 0.5801 - val_accuracy: 0.7567 - val_mae: 0.2277 - val_mse: 0.1125\n",
      "Epoch 4/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061\n",
      "Epoch 4: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061 - val_loss: 0.5563 - val_accuracy: 0.7680 - val_mae: 0.2159 - val_mse: 0.1071\n",
      "Epoch 5/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.7883 - mae: 0.1991 - mse: 0.0996\n",
      "Epoch 5: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996 - val_loss: 0.5310 - val_accuracy: 0.7846 - val_mae: 0.2024 - val_mse: 0.1025\n",
      "Epoch 6/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4890 - accuracy: 0.7979 - mae: 0.1894 - mse: 0.0946\n",
      "Epoch 6: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947 - val_loss: 0.5237 - val_accuracy: 0.7876 - val_mae: 0.2042 - val_mse: 0.1006\n",
      "Epoch 7/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4526 - accuracy: 0.8154 - mae: 0.1749 - mse: 0.0870\n",
      "Epoch 7: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870 - val_loss: 0.4840 - val_accuracy: 0.8034 - val_mae: 0.1841 - val_mse: 0.0922\n",
      "Epoch 8/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4288 - accuracy: 0.8248 - mae: 0.1666 - mse: 0.0828\n",
      "Epoch 8: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.4294 - accuracy: 0.8244 - mae: 0.1668 - mse: 0.0829 - val_loss: 0.4868 - val_accuracy: 0.8062 - val_mae: 0.2001 - val_mse: 0.0929\n",
      "Epoch 9/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8337 - mae: 0.1592 - mse: 0.0793\n",
      "Epoch 9: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.4113 - accuracy: 0.8334 - mae: 0.1593 - mse: 0.0794 - val_loss: 0.4809 - val_accuracy: 0.8056 - val_mae: 0.1955 - val_mse: 0.0918\n",
      "Epoch 10/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8391 - mae: 0.1530 - mse: 0.0761\n",
      "Epoch 10: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.3982 - accuracy: 0.8390 - mae: 0.1531 - mse: 0.0761 - val_loss: 0.4519 - val_accuracy: 0.8169 - val_mae: 0.1785 - val_mse: 0.0869\n",
      "Epoch 11/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8478 - mae: 0.1469 - mse: 0.0731\n",
      "Epoch 11: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731 - val_loss: 0.4487 - val_accuracy: 0.8235 - val_mae: 0.1769 - val_mse: 0.0858\n",
      "Epoch 12/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698\n",
      "Epoch 12: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698 - val_loss: 0.4570 - val_accuracy: 0.8199 - val_mae: 0.1743 - val_mse: 0.0876\n",
      "Epoch 13/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8614 - mae: 0.1328 - mse: 0.0661\n",
      "Epoch 13: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.3465 - accuracy: 0.8613 - mae: 0.1328 - mse: 0.0661 - val_loss: 0.4406 - val_accuracy: 0.8270 - val_mae: 0.1688 - val_mse: 0.0842\n",
      "Epoch 14/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8652 - mae: 0.1293 - mse: 0.0645\n",
      "Epoch 14: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.3365 - accuracy: 0.8646 - mae: 0.1296 - mse: 0.0647 - val_loss: 0.4417 - val_accuracy: 0.8217 - val_mae: 0.1649 - val_mse: 0.0841\n",
      "Epoch 15/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.8687 - mae: 0.1255 - mse: 0.0621\n",
      "Epoch 15: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3224 - accuracy: 0.8687 - mae: 0.1255 - mse: 0.0621 - val_loss: 0.4524 - val_accuracy: 0.8244 - val_mae: 0.1558 - val_mse: 0.0849\n",
      "Epoch 16/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607\n",
      "Epoch 16: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607 - val_loss: 0.4260 - val_accuracy: 0.8328 - val_mae: 0.1710 - val_mse: 0.0812\n",
      "Epoch 17/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8762 - mae: 0.1188 - mse: 0.0588\n",
      "Epoch 17: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.3062 - accuracy: 0.8760 - mae: 0.1188 - mse: 0.0589 - val_loss: 0.4412 - val_accuracy: 0.8255 - val_mae: 0.1547 - val_mse: 0.0830\n",
      "Epoch 18/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8807 - mae: 0.1135 - mse: 0.0572\n",
      "Epoch 18: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573 - val_loss: 0.4224 - val_accuracy: 0.8310 - val_mae: 0.1597 - val_mse: 0.0804\n",
      "Epoch 19/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552\n",
      "Epoch 19: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552 - val_loss: 0.4237 - val_accuracy: 0.8358 - val_mae: 0.1463 - val_mse: 0.0794\n",
      "Epoch 20/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.8848 - mae: 0.1100 - mse: 0.0555\n",
      "Epoch 20: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2880 - accuracy: 0.8850 - mae: 0.1100 - mse: 0.0555 - val_loss: 0.4052 - val_accuracy: 0.8440 - val_mae: 0.1515 - val_mse: 0.0765\n",
      "Epoch 21/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8901 - mae: 0.1056 - mse: 0.0533\n",
      "Epoch 21: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2783 - accuracy: 0.8903 - mae: 0.1055 - mse: 0.0533 - val_loss: 0.4240 - val_accuracy: 0.8358 - val_mae: 0.1556 - val_mse: 0.0801\n",
      "Epoch 22/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520\n",
      "Epoch 22: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520 - val_loss: 0.3986 - val_accuracy: 0.8425 - val_mae: 0.1510 - val_mse: 0.0759\n",
      "Epoch 23/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8979 - mae: 0.1005 - mse: 0.0503\n",
      "Epoch 23: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2632 - accuracy: 0.8979 - mae: 0.1004 - mse: 0.0503 - val_loss: 0.4111 - val_accuracy: 0.8446 - val_mae: 0.1445 - val_mse: 0.0772\n",
      "Epoch 24/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.8978 - mae: 0.0979 - mse: 0.0498\n",
      "Epoch 24: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2595 - accuracy: 0.8978 - mae: 0.0979 - mse: 0.0498 - val_loss: 0.4065 - val_accuracy: 0.8441 - val_mae: 0.1451 - val_mse: 0.0761\n",
      "Epoch 25/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479\n",
      "Epoch 25: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479 - val_loss: 0.4008 - val_accuracy: 0.8475 - val_mae: 0.1382 - val_mse: 0.0746\n",
      "Epoch 26/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9000 - mae: 0.0948 - mse: 0.0479\n",
      "Epoch 26: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2479 - accuracy: 0.9001 - mae: 0.0948 - mse: 0.0478 - val_loss: 0.4208 - val_accuracy: 0.8413 - val_mae: 0.1401 - val_mse: 0.0782\n",
      "Epoch 27/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9035 - mae: 0.0931 - mse: 0.0470\n",
      "Epoch 27: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2490 - accuracy: 0.9035 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.4033 - val_accuracy: 0.8405 - val_mae: 0.1423 - val_mse: 0.0760\n",
      "Epoch 27: early stopping\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.4113 - accuracy: 0.8373 - mae: 0.1439 - mse: 0.0774\n",
      "Val Score:  [0.41132697463035583, 0.8373253345489502, 0.14391790330410004, 0.0773804634809494]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  4\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 51)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.6368 - mae: 0.3168 - mse: 0.1596\n",
      "Epoch 1: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 10s 32ms/step - loss: 0.8196 - accuracy: 0.6368 - mae: 0.3168 - mse: 0.1596 - val_loss: 0.7131 - val_accuracy: 0.7039 - val_mae: 0.2918 - val_mse: 0.1376\n",
      "Epoch 2/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.7159 - mae: 0.2604 - mse: 0.1304\n",
      "Epoch 2: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.6770 - accuracy: 0.7162 - mae: 0.2602 - mse: 0.1303 - val_loss: 0.6272 - val_accuracy: 0.7414 - val_mae: 0.2568 - val_mse: 0.1210\n",
      "Epoch 3/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7510 - mae: 0.2308 - mse: 0.1154\n",
      "Epoch 3: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 22ms/step - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154 - val_loss: 0.5801 - val_accuracy: 0.7567 - val_mae: 0.2277 - val_mse: 0.1125\n",
      "Epoch 4/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061\n",
      "Epoch 4: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061 - val_loss: 0.5563 - val_accuracy: 0.7680 - val_mae: 0.2159 - val_mse: 0.1071\n",
      "Epoch 5/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996\n",
      "Epoch 5: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996 - val_loss: 0.5310 - val_accuracy: 0.7846 - val_mae: 0.2024 - val_mse: 0.1025\n",
      "Epoch 6/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947\n",
      "Epoch 6: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947 - val_loss: 0.5237 - val_accuracy: 0.7876 - val_mae: 0.2042 - val_mse: 0.1006\n",
      "Epoch 7/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4530 - accuracy: 0.8153 - mae: 0.1750 - mse: 0.0871\n",
      "Epoch 7: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870 - val_loss: 0.4840 - val_accuracy: 0.8034 - val_mae: 0.1841 - val_mse: 0.0922\n",
      "Epoch 8/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.8247 - mae: 0.1667 - mse: 0.0828\n",
      "Epoch 8: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.4294 - accuracy: 0.8244 - mae: 0.1668 - mse: 0.0829 - val_loss: 0.4868 - val_accuracy: 0.8062 - val_mae: 0.2001 - val_mse: 0.0929\n",
      "Epoch 9/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4111 - accuracy: 0.8337 - mae: 0.1592 - mse: 0.0793\n",
      "Epoch 9: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.4113 - accuracy: 0.8334 - mae: 0.1593 - mse: 0.0794 - val_loss: 0.4809 - val_accuracy: 0.8056 - val_mae: 0.1955 - val_mse: 0.0918\n",
      "Epoch 10/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8391 - mae: 0.1530 - mse: 0.0761\n",
      "Epoch 10: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.3982 - accuracy: 0.8390 - mae: 0.1531 - mse: 0.0761 - val_loss: 0.4519 - val_accuracy: 0.8169 - val_mae: 0.1785 - val_mse: 0.0869\n",
      "Epoch 11/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8478 - mae: 0.1470 - mse: 0.0731\n",
      "Epoch 11: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731 - val_loss: 0.4487 - val_accuracy: 0.8235 - val_mae: 0.1769 - val_mse: 0.0858\n",
      "Epoch 12/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698\n",
      "Epoch 12: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698 - val_loss: 0.4570 - val_accuracy: 0.8199 - val_mae: 0.1743 - val_mse: 0.0876\n",
      "Epoch 13/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8614 - mae: 0.1328 - mse: 0.0661\n",
      "Epoch 13: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3465 - accuracy: 0.8613 - mae: 0.1328 - mse: 0.0661 - val_loss: 0.4406 - val_accuracy: 0.8270 - val_mae: 0.1688 - val_mse: 0.0842\n",
      "Epoch 14/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8646 - mae: 0.1296 - mse: 0.0647\n",
      "Epoch 14: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.3365 - accuracy: 0.8646 - mae: 0.1296 - mse: 0.0647 - val_loss: 0.4417 - val_accuracy: 0.8217 - val_mae: 0.1649 - val_mse: 0.0841\n",
      "Epoch 15/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8687 - mae: 0.1256 - mse: 0.0621\n",
      "Epoch 15: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 31ms/step - loss: 0.3224 - accuracy: 0.8687 - mae: 0.1255 - mse: 0.0621 - val_loss: 0.4524 - val_accuracy: 0.8244 - val_mae: 0.1558 - val_mse: 0.0849\n",
      "Epoch 16/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607\n",
      "Epoch 16: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607 - val_loss: 0.4260 - val_accuracy: 0.8328 - val_mae: 0.1710 - val_mse: 0.0812\n",
      "Epoch 17/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8762 - mae: 0.1188 - mse: 0.0588\n",
      "Epoch 17: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3062 - accuracy: 0.8760 - mae: 0.1188 - mse: 0.0589 - val_loss: 0.4412 - val_accuracy: 0.8255 - val_mae: 0.1547 - val_mse: 0.0830\n",
      "Epoch 18/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573\n",
      "Epoch 18: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573 - val_loss: 0.4224 - val_accuracy: 0.8310 - val_mae: 0.1597 - val_mse: 0.0804\n",
      "Epoch 19/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.8867 - mae: 0.1112 - mse: 0.0553\n",
      "Epoch 19: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552 - val_loss: 0.4237 - val_accuracy: 0.8358 - val_mae: 0.1463 - val_mse: 0.0794\n",
      "Epoch 20/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8848 - mae: 0.1100 - mse: 0.0555\n",
      "Epoch 20: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2880 - accuracy: 0.8850 - mae: 0.1100 - mse: 0.0555 - val_loss: 0.4052 - val_accuracy: 0.8440 - val_mae: 0.1515 - val_mse: 0.0765\n",
      "Epoch 21/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8902 - mae: 0.1055 - mse: 0.0533\n",
      "Epoch 21: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2783 - accuracy: 0.8903 - mae: 0.1055 - mse: 0.0533 - val_loss: 0.4240 - val_accuracy: 0.8358 - val_mae: 0.1556 - val_mse: 0.0801\n",
      "Epoch 22/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520\n",
      "Epoch 22: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520 - val_loss: 0.3986 - val_accuracy: 0.8425 - val_mae: 0.1510 - val_mse: 0.0759\n",
      "Epoch 23/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.8979 - mae: 0.1004 - mse: 0.0503\n",
      "Epoch 23: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2632 - accuracy: 0.8979 - mae: 0.1004 - mse: 0.0503 - val_loss: 0.4111 - val_accuracy: 0.8446 - val_mae: 0.1445 - val_mse: 0.0772\n",
      "Epoch 24/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8977 - mae: 0.0980 - mse: 0.0498\n",
      "Epoch 24: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.2595 - accuracy: 0.8978 - mae: 0.0979 - mse: 0.0498 - val_loss: 0.4065 - val_accuracy: 0.8441 - val_mae: 0.1451 - val_mse: 0.0761\n",
      "Epoch 25/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9017 - mae: 0.0951 - mse: 0.0478\n",
      "Epoch 25: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479 - val_loss: 0.4008 - val_accuracy: 0.8475 - val_mae: 0.1382 - val_mse: 0.0746\n",
      "Epoch 26/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9000 - mae: 0.0948 - mse: 0.0479\n",
      "Epoch 26: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2479 - accuracy: 0.9001 - mae: 0.0948 - mse: 0.0478 - val_loss: 0.4208 - val_accuracy: 0.8413 - val_mae: 0.1401 - val_mse: 0.0782\n",
      "Epoch 27/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9034 - mae: 0.0932 - mse: 0.0471\n",
      "Epoch 27: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2490 - accuracy: 0.9035 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.4033 - val_accuracy: 0.8405 - val_mae: 0.1423 - val_mse: 0.0760\n",
      "Epoch 27: early stopping\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.4113 - accuracy: 0.8373 - mae: 0.1439 - mse: 0.0774\n",
      "Val Score:  [0.41132697463035583, 0.8373253345489502, 0.14391790330410004, 0.0773804634809494]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  5\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_12 (Conv1D)          (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 51)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.8208 - accuracy: 0.6359 - mae: 0.3171 - mse: 0.1599\n",
      "Epoch 1: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 25ms/step - loss: 0.8196 - accuracy: 0.6368 - mae: 0.3168 - mse: 0.1596 - val_loss: 0.7131 - val_accuracy: 0.7039 - val_mae: 0.2918 - val_mse: 0.1376\n",
      "Epoch 2/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.6775 - accuracy: 0.7159 - mae: 0.2604 - mse: 0.1304\n",
      "Epoch 2: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.6770 - accuracy: 0.7162 - mae: 0.2602 - mse: 0.1303 - val_loss: 0.6272 - val_accuracy: 0.7414 - val_mae: 0.2568 - val_mse: 0.1210\n",
      "Epoch 3/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154\n",
      "Epoch 3: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154 - val_loss: 0.5801 - val_accuracy: 0.7567 - val_mae: 0.2277 - val_mse: 0.1125\n",
      "Epoch 4/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061\n",
      "Epoch 4: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061 - val_loss: 0.5563 - val_accuracy: 0.7680 - val_mae: 0.2159 - val_mse: 0.1071\n",
      "Epoch 5/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996\n",
      "Epoch 5: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996 - val_loss: 0.5310 - val_accuracy: 0.7846 - val_mae: 0.2024 - val_mse: 0.1025\n",
      "Epoch 6/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947\n",
      "Epoch 6: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947 - val_loss: 0.5237 - val_accuracy: 0.7876 - val_mae: 0.2042 - val_mse: 0.1006\n",
      "Epoch 7/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870\n",
      "Epoch 7: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 31ms/step - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870 - val_loss: 0.4840 - val_accuracy: 0.8034 - val_mae: 0.1841 - val_mse: 0.0922\n",
      "Epoch 8/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.8247 - mae: 0.1667 - mse: 0.0828\n",
      "Epoch 8: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.4294 - accuracy: 0.8244 - mae: 0.1668 - mse: 0.0829 - val_loss: 0.4868 - val_accuracy: 0.8062 - val_mae: 0.2001 - val_mse: 0.0929\n",
      "Epoch 9/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.8334 - mae: 0.1593 - mse: 0.0794\n",
      "Epoch 9: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 31ms/step - loss: 0.4113 - accuracy: 0.8334 - mae: 0.1593 - mse: 0.0794 - val_loss: 0.4809 - val_accuracy: 0.8056 - val_mae: 0.1955 - val_mse: 0.0918\n",
      "Epoch 10/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8389 - mae: 0.1531 - mse: 0.0761\n",
      "Epoch 10: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3982 - accuracy: 0.8390 - mae: 0.1531 - mse: 0.0761 - val_loss: 0.4519 - val_accuracy: 0.8169 - val_mae: 0.1785 - val_mse: 0.0869\n",
      "Epoch 11/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731\n",
      "Epoch 11: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731 - val_loss: 0.4487 - val_accuracy: 0.8235 - val_mae: 0.1769 - val_mse: 0.0858\n",
      "Epoch 12/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3638 - accuracy: 0.8529 - mae: 0.1398 - mse: 0.0698\n",
      "Epoch 12: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 23ms/step - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698 - val_loss: 0.4570 - val_accuracy: 0.8199 - val_mae: 0.1743 - val_mse: 0.0876\n",
      "Epoch 13/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8614 - mae: 0.1328 - mse: 0.0661\n",
      "Epoch 13: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3465 - accuracy: 0.8613 - mae: 0.1328 - mse: 0.0661 - val_loss: 0.4406 - val_accuracy: 0.8270 - val_mae: 0.1688 - val_mse: 0.0842\n",
      "Epoch 14/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8652 - mae: 0.1293 - mse: 0.0645\n",
      "Epoch 14: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.3365 - accuracy: 0.8646 - mae: 0.1296 - mse: 0.0647 - val_loss: 0.4417 - val_accuracy: 0.8217 - val_mae: 0.1649 - val_mse: 0.0841\n",
      "Epoch 15/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8687 - mae: 0.1256 - mse: 0.0621\n",
      "Epoch 15: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.3224 - accuracy: 0.8687 - mae: 0.1255 - mse: 0.0621 - val_loss: 0.4524 - val_accuracy: 0.8244 - val_mae: 0.1558 - val_mse: 0.0849\n",
      "Epoch 16/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8746 - mae: 0.1211 - mse: 0.0606\n",
      "Epoch 16: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607 - val_loss: 0.4260 - val_accuracy: 0.8328 - val_mae: 0.1710 - val_mse: 0.0812\n",
      "Epoch 17/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8762 - mae: 0.1188 - mse: 0.0588\n",
      "Epoch 17: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.3062 - accuracy: 0.8760 - mae: 0.1188 - mse: 0.0589 - val_loss: 0.4412 - val_accuracy: 0.8255 - val_mae: 0.1547 - val_mse: 0.0830\n",
      "Epoch 18/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573\n",
      "Epoch 18: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 27ms/step - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573 - val_loss: 0.4224 - val_accuracy: 0.8310 - val_mae: 0.1597 - val_mse: 0.0804\n",
      "Epoch 19/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.8867 - mae: 0.1112 - mse: 0.0553\n",
      "Epoch 19: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552 - val_loss: 0.4237 - val_accuracy: 0.8358 - val_mae: 0.1463 - val_mse: 0.0794\n",
      "Epoch 20/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.8848 - mae: 0.1100 - mse: 0.0555\n",
      "Epoch 20: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2880 - accuracy: 0.8850 - mae: 0.1100 - mse: 0.0555 - val_loss: 0.4052 - val_accuracy: 0.8440 - val_mae: 0.1515 - val_mse: 0.0765\n",
      "Epoch 21/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8902 - mae: 0.1055 - mse: 0.0533\n",
      "Epoch 21: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2783 - accuracy: 0.8903 - mae: 0.1055 - mse: 0.0533 - val_loss: 0.4240 - val_accuracy: 0.8358 - val_mae: 0.1556 - val_mse: 0.0801\n",
      "Epoch 22/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520\n",
      "Epoch 22: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520 - val_loss: 0.3986 - val_accuracy: 0.8425 - val_mae: 0.1510 - val_mse: 0.0759\n",
      "Epoch 23/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8979 - mae: 0.1005 - mse: 0.0503\n",
      "Epoch 23: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.2632 - accuracy: 0.8979 - mae: 0.1004 - mse: 0.0503 - val_loss: 0.4111 - val_accuracy: 0.8446 - val_mae: 0.1445 - val_mse: 0.0772\n",
      "Epoch 24/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.8977 - mae: 0.0980 - mse: 0.0498\n",
      "Epoch 24: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.2595 - accuracy: 0.8978 - mae: 0.0979 - mse: 0.0498 - val_loss: 0.4065 - val_accuracy: 0.8441 - val_mae: 0.1451 - val_mse: 0.0761\n",
      "Epoch 25/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9017 - mae: 0.0951 - mse: 0.0478\n",
      "Epoch 25: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479 - val_loss: 0.4008 - val_accuracy: 0.8475 - val_mae: 0.1382 - val_mse: 0.0746\n",
      "Epoch 26/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9001 - mae: 0.0948 - mse: 0.0478\n",
      "Epoch 26: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.2479 - accuracy: 0.9001 - mae: 0.0948 - mse: 0.0478 - val_loss: 0.4208 - val_accuracy: 0.8413 - val_mae: 0.1401 - val_mse: 0.0782\n",
      "Epoch 27/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9034 - mae: 0.0932 - mse: 0.0471\n",
      "Epoch 27: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.2490 - accuracy: 0.9035 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.4033 - val_accuracy: 0.8405 - val_mae: 0.1423 - val_mse: 0.0760\n",
      "Epoch 27: early stopping\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.4113 - accuracy: 0.8373 - mae: 0.1439 - mse: 0.0774\n",
      "Val Score:  [0.41132697463035583, 0.8373253345489502, 0.14391790330410004, 0.0773804634809494]\n",
      "====================================================================================\n",
      "\n",
      "\n",
      "Training on Fold:  6\n",
      "\n",
      "Hyper Parameters\n",
      "\n",
      "Learning Rate: 0.0009\n",
      "Drop out: 0.38\n",
      "Batch dim: 64\n",
      "Number of epochs: 45\n",
      "\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_15 (Conv1D)          (None, 17, 128)           29696     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 17, 128)           0         \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 17, 64)            90176     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 17, 64)            0         \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 17, 3)             2115      \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 51)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 156       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,143\n",
      "Trainable params: 122,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.8208 - accuracy: 0.6359 - mae: 0.3171 - mse: 0.1599\n",
      "Epoch 1: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 26ms/step - loss: 0.8196 - accuracy: 0.6368 - mae: 0.3168 - mse: 0.1596 - val_loss: 0.7131 - val_accuracy: 0.7039 - val_mae: 0.2918 - val_mse: 0.1376\n",
      "Epoch 2/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6770 - accuracy: 0.7162 - mae: 0.2602 - mse: 0.1303\n",
      "Epoch 2: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.6770 - accuracy: 0.7162 - mae: 0.2602 - mse: 0.1303 - val_loss: 0.6272 - val_accuracy: 0.7414 - val_mae: 0.2568 - val_mse: 0.1210\n",
      "Epoch 3/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154\n",
      "Epoch 3: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.6000 - accuracy: 0.7510 - mae: 0.2307 - mse: 0.1154 - val_loss: 0.5801 - val_accuracy: 0.7567 - val_mae: 0.2277 - val_mse: 0.1125\n",
      "Epoch 4/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061\n",
      "Epoch 4: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.5518 - accuracy: 0.7715 - mae: 0.2129 - mse: 0.1061 - val_loss: 0.5563 - val_accuracy: 0.7680 - val_mae: 0.2159 - val_mse: 0.1071\n",
      "Epoch 5/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996\n",
      "Epoch 5: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.5151 - accuracy: 0.7881 - mae: 0.1991 - mse: 0.0996 - val_loss: 0.5310 - val_accuracy: 0.7846 - val_mae: 0.2024 - val_mse: 0.1025\n",
      "Epoch 6/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4890 - accuracy: 0.7979 - mae: 0.1894 - mse: 0.0946\n",
      "Epoch 6: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.4892 - accuracy: 0.7976 - mae: 0.1895 - mse: 0.0947 - val_loss: 0.5237 - val_accuracy: 0.7876 - val_mae: 0.2042 - val_mse: 0.1006\n",
      "Epoch 7/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4530 - accuracy: 0.8153 - mae: 0.1750 - mse: 0.0871\n",
      "Epoch 7: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.4526 - accuracy: 0.8155 - mae: 0.1749 - mse: 0.0870 - val_loss: 0.4840 - val_accuracy: 0.8034 - val_mae: 0.1841 - val_mse: 0.0922\n",
      "Epoch 8/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4288 - accuracy: 0.8248 - mae: 0.1666 - mse: 0.0828\n",
      "Epoch 8: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.4294 - accuracy: 0.8244 - mae: 0.1668 - mse: 0.0829 - val_loss: 0.4868 - val_accuracy: 0.8062 - val_mae: 0.2001 - val_mse: 0.0929\n",
      "Epoch 9/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4114 - accuracy: 0.8336 - mae: 0.1593 - mse: 0.0794\n",
      "Epoch 9: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.4113 - accuracy: 0.8334 - mae: 0.1593 - mse: 0.0794 - val_loss: 0.4809 - val_accuracy: 0.8056 - val_mae: 0.1955 - val_mse: 0.0918\n",
      "Epoch 10/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8389 - mae: 0.1531 - mse: 0.0761\n",
      "Epoch 10: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.3982 - accuracy: 0.8390 - mae: 0.1531 - mse: 0.0761 - val_loss: 0.4519 - val_accuracy: 0.8169 - val_mae: 0.1785 - val_mse: 0.0869\n",
      "Epoch 11/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8478 - mae: 0.1470 - mse: 0.0731\n",
      "Epoch 11: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3806 - accuracy: 0.8476 - mae: 0.1470 - mse: 0.0731 - val_loss: 0.4487 - val_accuracy: 0.8235 - val_mae: 0.1769 - val_mse: 0.0858\n",
      "Epoch 12/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698\n",
      "Epoch 12: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 24ms/step - loss: 0.3639 - accuracy: 0.8528 - mae: 0.1398 - mse: 0.0698 - val_loss: 0.4570 - val_accuracy: 0.8199 - val_mae: 0.1743 - val_mse: 0.0876\n",
      "Epoch 13/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8614 - mae: 0.1328 - mse: 0.0661\n",
      "Epoch 13: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.3465 - accuracy: 0.8613 - mae: 0.1328 - mse: 0.0661 - val_loss: 0.4406 - val_accuracy: 0.8270 - val_mae: 0.1688 - val_mse: 0.0842\n",
      "Epoch 14/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8647 - mae: 0.1295 - mse: 0.0647\n",
      "Epoch 14: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 32ms/step - loss: 0.3365 - accuracy: 0.8646 - mae: 0.1296 - mse: 0.0647 - val_loss: 0.4417 - val_accuracy: 0.8217 - val_mae: 0.1649 - val_mse: 0.0841\n",
      "Epoch 15/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8685 - mae: 0.1257 - mse: 0.0622\n",
      "Epoch 15: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.3224 - accuracy: 0.8687 - mae: 0.1255 - mse: 0.0621 - val_loss: 0.4524 - val_accuracy: 0.8244 - val_mae: 0.1558 - val_mse: 0.0849\n",
      "Epoch 16/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8742 - mae: 0.1212 - mse: 0.0607\n",
      "Epoch 16: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 26ms/step - loss: 0.3193 - accuracy: 0.8741 - mae: 0.1213 - mse: 0.0607 - val_loss: 0.4260 - val_accuracy: 0.8328 - val_mae: 0.1710 - val_mse: 0.0812\n",
      "Epoch 17/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8762 - mae: 0.1188 - mse: 0.0588\n",
      "Epoch 17: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 27ms/step - loss: 0.3062 - accuracy: 0.8760 - mae: 0.1188 - mse: 0.0589 - val_loss: 0.4412 - val_accuracy: 0.8255 - val_mae: 0.1547 - val_mse: 0.0830\n",
      "Epoch 18/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8803 - mae: 0.1136 - mse: 0.0573\n",
      "Epoch 18: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.2949 - accuracy: 0.8804 - mae: 0.1136 - mse: 0.0573 - val_loss: 0.4224 - val_accuracy: 0.8310 - val_mae: 0.1597 - val_mse: 0.0804\n",
      "Epoch 19/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552\n",
      "Epoch 19: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2885 - accuracy: 0.8872 - mae: 0.1109 - mse: 0.0552 - val_loss: 0.4237 - val_accuracy: 0.8358 - val_mae: 0.1463 - val_mse: 0.0794\n",
      "Epoch 20/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.8848 - mae: 0.1100 - mse: 0.0555\n",
      "Epoch 20: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 29ms/step - loss: 0.2880 - accuracy: 0.8850 - mae: 0.1100 - mse: 0.0555 - val_loss: 0.4052 - val_accuracy: 0.8440 - val_mae: 0.1515 - val_mse: 0.0765\n",
      "Epoch 21/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8902 - mae: 0.1055 - mse: 0.0533\n",
      "Epoch 21: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2783 - accuracy: 0.8903 - mae: 0.1055 - mse: 0.0533 - val_loss: 0.4240 - val_accuracy: 0.8358 - val_mae: 0.1556 - val_mse: 0.0801\n",
      "Epoch 22/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8921 - mae: 0.1033 - mse: 0.0519\n",
      "Epoch 22: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 7s 25ms/step - loss: 0.2717 - accuracy: 0.8918 - mae: 0.1035 - mse: 0.0520 - val_loss: 0.3986 - val_accuracy: 0.8425 - val_mae: 0.1510 - val_mse: 0.0759\n",
      "Epoch 23/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.8977 - mae: 0.1005 - mse: 0.0504\n",
      "Epoch 23: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 6s 23ms/step - loss: 0.2632 - accuracy: 0.8979 - mae: 0.1004 - mse: 0.0503 - val_loss: 0.4111 - val_accuracy: 0.8446 - val_mae: 0.1445 - val_mse: 0.0772\n",
      "Epoch 24/45\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.8977 - mae: 0.0980 - mse: 0.0498\n",
      "Epoch 24: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 27ms/step - loss: 0.2595 - accuracy: 0.8978 - mae: 0.0979 - mse: 0.0498 - val_loss: 0.4065 - val_accuracy: 0.8441 - val_mae: 0.1451 - val_mse: 0.0761\n",
      "Epoch 25/45\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479\n",
      "Epoch 25: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 8s 28ms/step - loss: 0.2497 - accuracy: 0.9014 - mae: 0.0952 - mse: 0.0479 - val_loss: 0.4008 - val_accuracy: 0.8475 - val_mae: 0.1382 - val_mse: 0.0746\n",
      "Epoch 26/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9000 - mae: 0.0948 - mse: 0.0479\n",
      "Epoch 26: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 32ms/step - loss: 0.2479 - accuracy: 0.9001 - mae: 0.0948 - mse: 0.0478 - val_loss: 0.4208 - val_accuracy: 0.8413 - val_mae: 0.1401 - val_mse: 0.0782\n",
      "Epoch 27/45\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9034 - mae: 0.0932 - mse: 0.0471\n",
      "Epoch 27: val_loss did not improve from 0.39604\n",
      "282/282 [==============================] - 9s 31ms/step - loss: 0.2490 - accuracy: 0.9035 - mae: 0.0931 - mse: 0.0470 - val_loss: 0.4033 - val_accuracy: 0.8405 - val_mae: 0.1423 - val_mse: 0.0760\n",
      "Epoch 27: early stopping\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.4113 - accuracy: 0.8373 - mae: 0.1439 - mse: 0.0774\n",
      "Val Score:  [0.41132697463035583, 0.8373253345489502, 0.14391790330410004, 0.0773804634809494]\n",
      "====================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('fas_mnist_1_6f.h5', verbose=1, save_best_only=True)\n",
    "n_folds=6\n",
    "epochs = 45\n",
    "batch_size=64\n",
    "\n",
    "#save the model history in a list after fitting so that we can plot later\n",
    "model_history = []\n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    t_x, test_x, t_y, test_y = train_test_split(X, y, test_size=0.2,\n",
    "                                               random_state = np.random.randint(1,1000, 1)[0])\n",
    "    model_history.append(fit_and_evaluate(t_x, test_x, t_y, test_y, epochs, batch_size))\n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.average([0.8373253345489502, 0.8373253345489502, 0.8373253345489502, 0.8373253345489502, ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.40677431, 0.8521291 , 0.12807888, 0.07363142],\n       [0.41132697, 0.83732533, 0.1439179 , 0.07738046],\n       [0.41132697, 0.83732533, 0.1439179 , 0.07738046],\n       [0.41132697, 0.83732533, 0.1439179 , 0.07738046],\n       [0.41132697, 0.83732533, 0.1439179 , 0.07738046],\n       [0.41132697, 0.83732533, 0.1439179 , 0.07738046]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_array = np.array(model_history)\n",
    "model_array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average stratified 6-fold cross-validation score with knn classifier:0.8398\n",
      "Average stratified 6-fold cross-validation MAE with knn classifier:0.1413\n",
      "Average stratified 6-fold cross-validation MsE with knn classifier:0.0768\n"
     ]
    }
   ],
   "source": [
    "print('Average stratified 6-fold cross-validation score with knn classifier:{:.4f}'.format(model_array[:,1].mean()))\n",
    "print('Average stratified 6-fold cross-validation MAE with knn classifier:{:.4f}'.format(model_array[:,2].mean()))\n",
    "print('Average stratified 6-fold cross-validation MsE with knn classifier:{:.4f}'.format(model_array[:,3].mean()))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
